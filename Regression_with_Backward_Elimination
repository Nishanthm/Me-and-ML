import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df=pd.read_csv('50_Startups.csv')
X=df.iloc[:,:-1].values
y=df.iloc[:,4].values

from sklearn.preprocessing import LabelEncoder,OneHotEncoder
le=LabelEncoder()
X[:,3]=le.fit_transform(X[:,3])
ohe=OneHotEncoder(categorical_features=[3])
X=ohe.fit_transform(X).toarray()

X=X[:,1:]
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

from sklearn.linear_model import LinearRegression
model=LinearRegression()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

#Now we will peerform backward elimination to find out the strongest predictors of the model.

import statsmodels.api as sm
X=np.append(np.ones((50,1)).astype(int),X,axis=1)

X_opt=X[:,[0,1,2,3,4,5]]
model_OLS=sm.OLS(endog=y,exog=X_opt).fit()
model_OLS.summary()

X_opt=X[:,[0,1,3,4,5]]
model_OLS=sm.OLS(endog=y,exog=X_opt).fit()
model_OLS.summary()

X_opt=X[:,[0,3,4,5]]
model_OLS=sm.OLS(endog=y,exog=X_opt).fit()
model_OLS.summary()

X_opt=X[:,[0,3,5]]
model_OLS=sm.OLS(endog=y,exog=X_opt).fit()
model_OLS.summary()

X_opt=X[:,[0,3]]
model_OLS=sm.OLS(endog=y,exog=X_opt).fit()
model_OLS.summary()

#Finally we are left with only one feature, i.e with index no. as 3 which is the strongest predictore for the model.
